{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021668,
     "end_time": "2021-12-06T08:00:03.133146",
     "exception": false,
     "start_time": "2021-12-06T08:00:03.111478",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction \n",
    "\n",
    "In this notebook, I demonstrate how to use the multiclass focal loss that should help you score better with such imbalanced classes. The focal loss function is from https://github.com/artemmavrin/focal-loss/blob/master/docs/source/index.rst\n",
    "\n",
    "The focal loss is a loss that has been devised for object detection problems where the background is more prominent than the objects to be detected. \n",
    "\n",
    "![](https://github.com/Atomwh/FocalLoss_Keras/raw/master/images/fig1-focal%20loss%20results.png)\n",
    "\n",
    "As you increase the gamma value, you put more emphasis on hard to classify examples. There is clearly a trade-off for this (high gamma values can be detrimental), but overall if you set the right value it should perform much better than using other tricks for imbalanced data.\n",
    "\n",
    "In order to implement the multiclass focal loss, I referred to the articles below: \n",
    "\n",
    "- https://paperswithcode.com/method/focal-loss\n",
    "- https://amaarora.github.io/2020/06/29/FocalLoss.html\n",
    "- https://medium.com/swlh/focal-loss-what-why-and-how-df6735f26616\n",
    "\n",
    "This notebook owes quite a lot of ideas from \"TPSDEC21-01-Keras Quickstart\" (https://www.kaggle.com/ambrosm/tpsdec21-01-keras-quickstart) by @ambrosm .\n",
    "\n",
    "It also implements the feature engineering suggested by @aguschin (see the post https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/291839 for all the references).\n",
    "\n",
    "**Note**: the main flow of the notebook inspired by https://www.kaggle.com/lucamassaron/lightgbm-with-multiclass-focal-loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T08:00:03.173961Z",
     "iopub.status.busy": "2021-12-06T08:00:03.169508Z",
     "iopub.status.idle": "2021-12-06T08:00:11.520374Z",
     "shell.execute_reply": "2021-12-06T08:00:11.519619Z",
     "shell.execute_reply.started": "2021-12-04T15:30:17.326804Z"
    },
    "papermill": {
     "duration": 8.37023,
     "end_time": "2021-12-06T08:00:11.520556",
     "exception": false,
     "start_time": "2021-12-06T08:00:03.150326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n",
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from warnings import filterwarnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import lightgbm as lgbm\n",
    "\n",
    "\n",
    "\n",
    "filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at  2021-12-06 15:15:28.710745\n"
     ]
    }
   ],
   "source": [
    "# main flow\n",
    "import datetime as dt\n",
    "start_time = dt.datetime.now()\n",
    "print(\"Started at \", start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T08:00:11.566051Z",
     "iopub.status.busy": "2021-12-06T08:00:11.564892Z",
     "iopub.status.idle": "2021-12-06T08:00:11.573312Z",
     "shell.execute_reply": "2021-12-06T08:00:11.573864Z",
     "shell.execute_reply.started": "2021-12-04T15:30:25.081074Z"
    },
    "papermill": {
     "duration": 0.034518,
     "end_time": "2021-12-06T08:00:11.574059",
     "exception": false,
     "start_time": "2021-12-06T08:00:11.539541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T08:00:11.612942Z",
     "iopub.status.busy": "2021-12-06T08:00:11.611756Z",
     "iopub.status.idle": "2021-12-06T08:00:11.630315Z",
     "shell.execute_reply": "2021-12-06T08:00:11.630847Z",
     "shell.execute_reply.started": "2021-12-04T15:30:25.095102Z"
    },
    "papermill": {
     "duration": 0.039752,
     "end_time": "2021-12-06T08:00:11.631050",
     "exception": false,
     "start_time": "2021-12-06T08:00:11.591298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "from scipy import special\n",
    "\n",
    "class FocalLoss:\n",
    "    \"\"\"\n",
    "    source: https://maxhalford.github.io/blog/lightgbm-focal-loss/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma, alpha=None):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def at(self, y):\n",
    "        if self.alpha is None:\n",
    "            return np.ones_like(y)\n",
    "        return np.where(y, self.alpha, 1 - self.alpha)\n",
    "\n",
    "    def pt(self, y, p):\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return np.where(y, p, 1 - p)\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        at = self.at(y_true)\n",
    "        pt = self.pt(y_true, y_pred)\n",
    "        return -at * (1 - pt) ** self.gamma * np.log(pt)\n",
    "\n",
    "    def grad(self, y_true, y_pred):\n",
    "        y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n",
    "        at = self.at(y_true)\n",
    "        pt = self.pt(y_true, y_pred)\n",
    "        g = self.gamma\n",
    "        return at * y * (1 - pt) ** g * (g * pt * np.log(pt) + pt - 1)\n",
    "\n",
    "    def hess(self, y_true, y_pred):\n",
    "        y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n",
    "        at = self.at(y_true)\n",
    "        pt = self.pt(y_true, y_pred)\n",
    "        g = self.gamma\n",
    "\n",
    "        u = at * y * (1 - pt) ** g\n",
    "        du = -at * y * g * (1 - pt) ** (g - 1)\n",
    "        v = g * pt * np.log(pt) + pt - 1\n",
    "        dv = g * np.log(pt) + g + 1\n",
    "\n",
    "        return (du * v + u * dv) * y * (pt * (1 - pt))\n",
    "\n",
    "    def init_score(self, y_true):\n",
    "        res = optimize.minimize_scalar(\n",
    "            lambda p: self(y_true, p).sum(),\n",
    "            bounds=(0, 1),\n",
    "            method='bounded'\n",
    "        )\n",
    "        p = res.x\n",
    "        log_odds = np.log(p / (1 - p))\n",
    "        return log_odds\n",
    "\n",
    "    def lgb_obj(self, preds, train_data):\n",
    "        y = train_data.get_label()\n",
    "        p = special.expit(preds)\n",
    "        return self.grad(y, p), self.hess(y, p)\n",
    "\n",
    "    def lgb_eval(self, preds, train_data):\n",
    "        y = train_data.get_label()\n",
    "        p = special.expit(preds)\n",
    "        is_higher_better = False\n",
    "        return 'focal_loss', self(y, p).mean(), is_higher_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T08:00:11.668983Z",
     "iopub.status.busy": "2021-12-06T08:00:11.667802Z",
     "iopub.status.idle": "2021-12-06T08:00:11.697876Z",
     "shell.execute_reply": "2021-12-06T08:00:11.698460Z",
     "shell.execute_reply.started": "2021-12-04T15:30:25.119093Z"
    },
    "papermill": {
     "duration": 0.050856,
     "end_time": "2021-12-06T08:00:11.698667",
     "exception": false,
     "start_time": "2021-12-06T08:00:11.647811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sklearn.multiclass import _ConstantPredictor\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy import special\n",
    "\n",
    "\n",
    "class OneVsRestLightGBMWithCustomizedLoss:\n",
    "    \"\"\"\n",
    "    source: https://towardsdatascience.com/multi-class-classification-using-focal-loss-and-lightgbm-a6a6dec28872\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss, n_jobs=3):\n",
    "        self.loss = loss\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y, **fit_params):\n",
    "\n",
    "        self.label_binarizer_ = LabelBinarizer(sparse_output=True)\n",
    "        Y = self.label_binarizer_.fit_transform(y)\n",
    "        Y = Y.tocsc()\n",
    "        self.classes_ = self.label_binarizer_.classes_\n",
    "        columns = (col.toarray().ravel() for col in Y.T)\n",
    "        if 'eval_set' in fit_params:\n",
    "            # use eval_set for early stopping\n",
    "            X_val, y_val = fit_params['eval_set'][0]\n",
    "            Y_val = self.label_binarizer_.transform(y_val)\n",
    "            Y_val = Y_val.tocsc()\n",
    "            columns_val = (col.toarray().ravel() for col in Y_val.T)\n",
    "            self.results_ = Parallel(n_jobs=self.n_jobs)(delayed(self._fit_binary)\n",
    "                                                         (X, column, X_val, column_val, **fit_params) for\n",
    "                                                         i, (column, column_val) in\n",
    "                                                         enumerate(zip(columns, columns_val)))\n",
    "        else:\n",
    "            # eval set not available\n",
    "            self.results_ = Parallel(n_jobs=self.n_jobs)(delayed(self._fit_binary)\n",
    "                                                         (X, column, None, None, **fit_params) for i, column\n",
    "                                                         in enumerate(columns))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _fit_binary(self, X, y, X_val, y_val, **fit_params):\n",
    "        unique_y = np.unique(y)\n",
    "        init_score_value = self.loss.init_score(y)\n",
    "        if len(unique_y) == 1:\n",
    "            estimator = _ConstantPredictor().fit(X, unique_y)\n",
    "        else:\n",
    "            fit = lgbm.Dataset(X, y, init_score=np.full_like(y, init_score_value, dtype=float))\n",
    "            filtering = ['eval_set', 'early_stopping_rounds', 'verbose_eval', 'num_boost_round']\n",
    "            local_fit_params = {item:value for item, value in fit_params.items() if item!='eval_set'}\n",
    "            \n",
    "            if 'num_boost_round' in fit_params:\n",
    "                num_boost_round = fit_params['num_boost_round']\n",
    "            else:\n",
    "                num_boost_round = 100\n",
    "                \n",
    "            if 'early_stopping_rounds' in fit_params:\n",
    "                early_stopping_rounds = fit_params['early_stopping_rounds']\n",
    "            else:\n",
    "                early_stopping_rounds = 10\n",
    "                \n",
    "            if 'verbose_eval'  in fit_params:\n",
    "                verbose_eval = fit_params['verbose_eval']\n",
    "            else:\n",
    "                verbose_eval = 10\n",
    "                    \n",
    "            if 'eval_set' in fit_params:\n",
    "                val = lgbm.Dataset(X_val, y_val, init_score=np.full_like(y_val, init_score_value, dtype=float),\n",
    "                                  reference=fit)\n",
    "        \n",
    "                estimator = lgbm.train(params=local_fit_params,\n",
    "                                       train_set=fit,\n",
    "                                       valid_sets=(fit, val),\n",
    "                                       valid_names=('fit', 'val'),\n",
    "                                       fobj=self.loss.lgb_obj,\n",
    "                                       feval=self.loss.lgb_eval,\n",
    "                                       num_boost_round=num_boost_round,\n",
    "                                       early_stopping_rounds=early_stopping_rounds,\n",
    "                                       verbose_eval=verbose_eval)\n",
    "            else:\n",
    "                                   \n",
    "                estimator = lgbm.train(params=local_fit_params,\n",
    "                                       train_set=fit,\n",
    "                                       fobj=self.loss.lgb_obj,\n",
    "                                       feval=self.loss.lgb_eval,\n",
    "                                       num_boost_round=num_boost_round,\n",
    "                                       early_stopping_rounds=early_stopping_rounds,\n",
    "                                       verbose_eval=verbose_eval)\n",
    "\n",
    "        return estimator, init_score_value\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        maxima = np.empty(n_samples, dtype=float)\n",
    "        maxima.fill(-np.inf)\n",
    "        argmaxima = np.zeros(n_samples, dtype=int)\n",
    "\n",
    "        for i, (e, init_score) in enumerate(self.results_):\n",
    "            margins = e.predict(X, raw_score=True)\n",
    "            prob = special.expit(margins + init_score)\n",
    "            np.maximum(maxima, prob, out=maxima)\n",
    "            argmaxima[maxima == prob] = i\n",
    "\n",
    "        return argmaxima\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        y = np.zeros((X.shape[0], len(self.results_)))\n",
    "        for i, (e, init_score) in enumerate(self.results_):\n",
    "            margins = e.predict(X, raw_score=True)\n",
    "            y[:, i] = special.expit(margins + init_score)\n",
    "        y /= np.sum(y, axis=1)[:, np.newaxis]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict\n",
    "\n",
    "# read data\n",
    "in_kaggle = False\n",
    "\n",
    "def get_data_file_path(is_in_kaggle: bool) -> Tuple[str, str, str]:\n",
    "    train_path = ''\n",
    "    test_path = ''\n",
    "    sample_submission_path = ''\n",
    "\n",
    "    if is_in_kaggle:\n",
    "        # running in Kaggle, inside the competition\n",
    "        train_path = '../input/tabular-playground-series-dec-2021/train.csv'\n",
    "        test_path = '../input/tabular-playground-series-dec-2021/test.csv'\n",
    "        sample_submission_path = '../input/tabular-playground-series-dec-2021/sample_submission.csv'\n",
    "    else:\n",
    "        # running locally\n",
    "        train_path = 'data/train.csv'\n",
    "        test_path = 'data/test.csv'\n",
    "        sample_submission_path = 'data/sample_submission.csv'\n",
    "\n",
    "    return train_path, test_path, sample_submission_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T08:00:11.736288Z",
     "iopub.status.busy": "2021-12-06T08:00:11.735166Z",
     "iopub.status.idle": "2021-12-06T08:00:33.848937Z",
     "shell.execute_reply": "2021-12-06T08:00:33.848341Z",
     "shell.execute_reply.started": "2021-12-04T15:30:25.14882Z"
    },
    "papermill": {
     "duration": 22.13377,
     "end_time": "2021-12-06T08:00:33.849274",
     "exception": false,
     "start_time": "2021-12-06T08:00:11.715504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path, test_path, sample_submission_path = get_data_file_path(in_kaggle)\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "submission = pd.read_csv(sample_submission_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T08:00:33.887546Z",
     "iopub.status.busy": "2021-12-06T08:00:33.886817Z",
     "iopub.status.idle": "2021-12-06T08:00:34.712314Z",
     "shell.execute_reply": "2021-12-06T08:00:34.711589Z",
     "shell.execute_reply.started": "2021-12-04T15:30:47.231779Z"
    },
    "papermill": {
     "duration": 0.845793,
     "end_time": "2021-12-06T08:00:34.712471",
     "exception": false,
     "start_time": "2021-12-06T08:00:33.866678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target class distribution:\n",
      "Cover_Type\n",
      "1    36.703%\n",
      "2    56.552%\n",
      "3     4.893%\n",
      "4     0.009%\n",
      "5     0.000%\n",
      "6     0.286%\n",
      "7     1.557%\n",
      "Name: Id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"The target class distribution:\")\n",
    "print((train.groupby('Cover_Type').Id.nunique() / len(train)).apply(lambda p: f\"{p:.3%}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T08:00:34.751969Z",
     "iopub.status.busy": "2021-12-06T08:00:34.751246Z",
     "iopub.status.idle": "2021-12-06T08:00:35.655189Z",
     "shell.execute_reply": "2021-12-06T08:00:35.655762Z",
     "shell.execute_reply.started": "2021-12-04T15:30:48.035439Z"
    },
    "papermill": {
     "duration": 0.926213,
     "end_time": "2021-12-06T08:00:35.655954",
     "exception": false,
     "start_time": "2021-12-06T08:00:34.729741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Droping Cover_Type 5 label, since there is only one instance of it\n",
    "train = train[train.Cover_Type != 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T08:00:35.699989Z",
     "iopub.status.busy": "2021-12-06T08:00:35.699308Z",
     "iopub.status.idle": "2021-12-06T08:00:46.403868Z",
     "shell.execute_reply": "2021-12-06T08:00:46.403286Z",
     "shell.execute_reply.started": "2021-12-04T15:30:48.933249Z"
    },
    "papermill": {
     "duration": 10.730658,
     "end_time": "2021-12-06T08:00:46.404030",
     "exception": false,
     "start_time": "2021-12-06T08:00:35.673372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove unuseful features\n",
    "train = train.drop([ 'Soil_Type7', 'Soil_Type15'], axis=1)\n",
    "test = test.drop(['Soil_Type7', 'Soil_Type15'], axis=1)\n",
    "\n",
    "# extra feature engineering\n",
    "def r(x):\n",
    "    if x+180>360:\n",
    "        return x-180\n",
    "    else:\n",
    "        return x+180\n",
    "\n",
    "def fe(df):\n",
    "    df['EHiElv'] = df['Horizontal_Distance_To_Roadways'] * df['Elevation']\n",
    "    df['EViElv'] = df['Vertical_Distance_To_Hydrology'] * df['Elevation']\n",
    "    df['Aspect2'] = df.Aspect.map(r)\n",
    "    ### source: https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/293373\n",
    "    df[\"Aspect\"][df[\"Aspect\"] < 0] += 360\n",
    "    df[\"Aspect\"][df[\"Aspect\"] > 359] -= 360\n",
    "    df.loc[df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n",
    "    df.loc[df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n",
    "    df.loc[df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n",
    "    df.loc[df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n",
    "    df.loc[df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n",
    "    df.loc[df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n",
    "    ########\n",
    "    df['Highwater'] = (df.Vertical_Distance_To_Hydrology < 0).astype(int)\n",
    "    df['EVDtH'] = df.Elevation - df.Vertical_Distance_To_Hydrology\n",
    "    df['EHDtH'] = df.Elevation - df.Horizontal_Distance_To_Hydrology * 0.2\n",
    "    df['Euclidean_Distance_to_Hydrolody'] = (df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)**0.5\n",
    "    df['Manhattan_Distance_to_Hydrolody'] = df['Horizontal_Distance_To_Hydrology'] + df['Vertical_Distance_To_Hydrology']\n",
    "    df['Hydro_Fire_1'] = df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points']\n",
    "    df['Hydro_Fire_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points'])\n",
    "    df['Hydro_Road_1'] = abs(df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways'])\n",
    "    df['Hydro_Road_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways'])\n",
    "    df['Fire_Road_1'] = abs(df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways'])\n",
    "    df['Fire_Road_2'] = abs(df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways'])\n",
    "    df['Hillshade_3pm_is_zero'] = (df.Hillshade_3pm == 0).astype(int)\n",
    "    return df\n",
    "\n",
    "train = fe(train)\n",
    "test = fe(test)\n",
    "\n",
    "# Summed features pointed out by @craigmthomas (https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/292823)\n",
    "soil_features = [x for x in train.columns if x.startswith(\"Soil_Type\")]\n",
    "wilderness_features = [x for x in train.columns if x.startswith(\"Wilderness_Area\")]\n",
    "\n",
    "train[\"soil_type_count\"] = train[soil_features].sum(axis=1)\n",
    "test[\"soil_type_count\"] = test[soil_features].sum(axis=1)\n",
    "\n",
    "train[\"wilderness_area_count\"] = train[wilderness_features].sum(axis=1)\n",
    "test[\"wilderness_area_count\"] = test[wilderness_features].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T08:00:46.445846Z",
     "iopub.status.busy": "2021-12-06T08:00:46.445164Z",
     "iopub.status.idle": "2021-12-06T08:01:21.521596Z",
     "shell.execute_reply": "2021-12-06T08:01:21.522191Z",
     "shell.execute_reply.started": "2021-12-04T15:30:53.634253Z"
    },
    "papermill": {
     "duration": 35.101077,
     "end_time": "2021-12-06T08:01:21.522395",
     "exception": false,
     "start_time": "2021-12-06T08:00:46.421318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 583.43 Mb (74.6% reduction)\n",
      "Mem. usage decreased to 98.23 Mb (81.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "y = train.Cover_Type.values - 1\n",
    "X = reduce_mem_usage(train.drop(\"Cover_Type\", axis=1)).set_index(\"Id\")\n",
    "Xt = reduce_mem_usage(test).set_index(\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T08:01:21.932573Z",
     "iopub.status.busy": "2021-12-06T08:01:21.568502Z",
     "iopub.status.idle": "2021-12-06T08:01:22.473722Z",
     "shell.execute_reply": "2021-12-06T08:01:22.473110Z",
     "shell.execute_reply.started": "2021-12-04T15:31:22.83336Z"
    },
    "papermill": {
     "duration": 0.933216,
     "end_time": "2021-12-06T08:01:22.473881",
     "exception": false,
     "start_time": "2021-12-06T08:01:21.540665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del([train, test])\n",
    "_ = [gc.collect() for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T08:01:22.515798Z",
     "iopub.status.busy": "2021-12-06T08:01:22.515106Z",
     "iopub.status.idle": "2021-12-06T08:01:22.805324Z",
     "shell.execute_reply": "2021-12-06T08:01:22.804701Z",
     "shell.execute_reply.started": "2021-12-04T15:31:23.737208Z"
    },
    "papermill": {
     "duration": 0.313083,
     "end_time": "2021-12-06T08:01:22.805498",
     "exception": false,
     "start_time": "2021-12-06T08:01:22.492415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "target = le.fit_transform(y)\n",
    "\n",
    "_, classes_num = np.unique(target, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T08:01:22.853419Z",
     "iopub.status.busy": "2021-12-06T08:01:22.852678Z",
     "iopub.status.idle": "2021-12-06T11:55:19.966058Z",
     "shell.execute_reply": "2021-12-06T11:55:19.966918Z",
     "shell.execute_reply.started": "2021-12-04T15:31:24.038546Z"
    },
    "papermill": {
     "duration": 14037.144551,
     "end_time": "2021-12-06T11:55:19.968137",
     "exception": false,
     "start_time": "2021-12-06T08:01:22.823586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "Fold 1 || Training\n",
      "****************************************\n",
      "cv accuracy fold 1: 0.96252\n",
      "****************************************\n",
      "Fold 2 || Training\n",
      "****************************************\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.65 GiB for an array with shape (3199999, 69) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"<ipython-input-5-155b03941350>\", line 78, in _fit_binary\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\", line 228, in train\n    booster = Booster(params=params, train_set=train_set)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 1714, in __init__\n    train_set.construct().handle,\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 1085, in construct\n    categorical_feature=self.categorical_feature, params=self.params)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 887, in _lazy_init\n    self.__init_from_np2d(data, params_str, ref_dataset)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 924, in __init_from_np2d\n    data = np.array(mat.reshape(mat.size), dtype=mat.dtype, copy=False)\nMemoryError: Unable to allocate 1.65 GiB for an array with shape (3199999, 69) and data type float64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-21487e6f57bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'**'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mN_FOLDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-155b03941350>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m     30\u001b[0m                                                          \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                                                          \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_val\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                                                          enumerate(zip(columns, columns_val)))\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;31m# eval set not available\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    423\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.65 GiB for an array with shape (3199999, 69) and data type float64"
     ]
    }
   ],
   "source": [
    "N_FOLDS = 5\n",
    "\n",
    "### cross-validation \n",
    "cv = KFold(n_splits=N_FOLDS, shuffle=True, random_state=1)\n",
    "\n",
    "predictions = np.zeros((len(Xt), len(le.classes_)))\n",
    "oof = np.zeros((len(X), len(le.classes_)))\n",
    "scores = list()\n",
    "\n",
    "for fold, (idx_train, idx_valid) in enumerate(cv.split(X, y)):\n",
    "    X_train, y_train = X.iloc[idx_train, :], target[idx_train]\n",
    "    X_valid, y_valid = X.iloc[idx_valid, :], target[idx_valid]\n",
    "    \n",
    "    fit_params = {'eval_set': [(X_valid, y_valid)],\n",
    "                  'num_boost_round': 1500,\n",
    "                  'early_stopping_rounds': 30,\n",
    "                  'verbose_eval': 100\n",
    "                 }\n",
    "    \n",
    "    loss = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "    model = OneVsRestLightGBMWithCustomizedLoss(loss=loss)\n",
    "\n",
    "    print('**'*20)\n",
    "    print(f\"Fold {fold+1} || Training\")\n",
    "    print('**'*20)\n",
    "\n",
    "    model.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "    predictions += model.predict_proba(Xt) / N_FOLDS\n",
    "    oof[idx_valid] = model.predict_proba(X_valid)\n",
    "        \n",
    "    scores.append(accuracy_score(y_true=y_valid, y_pred=np.argmax(oof[idx_valid], axis=1)))\n",
    "    print(f\"cv accuracy fold {fold+1}: {scores[-1]:0.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T11:55:20.034130Z",
     "iopub.status.busy": "2021-12-06T11:55:20.032995Z",
     "iopub.status.idle": "2021-12-06T11:55:20.046117Z",
     "shell.execute_reply": "2021-12-06T11:55:20.046789Z",
     "shell.execute_reply.started": "2021-12-04T16:22:24.287087Z"
    },
    "papermill": {
     "duration": 0.050658,
     "end_time": "2021-12-06T11:55:20.046979",
     "exception": false,
     "start_time": "2021-12-06T11:55:19.996321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Average cv accuracy: {np.mean(scores):0.5f} (std={np.std(scores):0.5f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T11:55:20.323252Z",
     "iopub.status.busy": "2021-12-06T11:55:20.321993Z",
     "iopub.status.idle": "2021-12-06T11:55:22.201508Z",
     "shell.execute_reply": "2021-12-06T11:55:22.202447Z",
     "shell.execute_reply.started": "2021-12-04T16:22:24.28941Z"
    },
    "papermill": {
     "duration": 2.127645,
     "end_time": "2021-12-06T11:55:22.202781",
     "exception": false,
     "start_time": "2021-12-06T11:55:20.075136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.Cover_Type = le.inverse_transform(np.argmax(predictions, axis=1)) + 1\n",
    "submission.to_csv(\"output/light_gbm_focal_loss/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T11:55:22.272915Z",
     "iopub.status.busy": "2021-12-06T11:55:22.272181Z",
     "iopub.status.idle": "2021-12-06T11:56:15.493618Z",
     "shell.execute_reply": "2021-12-06T11:56:15.494312Z",
     "shell.execute_reply.started": "2021-12-04T16:22:24.29066Z"
    },
    "papermill": {
     "duration": 53.258973,
     "end_time": "2021-12-06T11:56:15.494535",
     "exception": false,
     "start_time": "2021-12-06T11:55:22.235562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oof = pd.DataFrame(oof, columns=[f\"prob_{i}\" for i in le.classes_])\n",
    "oof.insert(loc=0, column='Id', value=range(len(X)))\n",
    "oof.to_csv(\"output/light_gbm_focal_loss/oof.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We are done. That is all, folks!')\n",
    "finish_time = dt.datetime.now()\n",
    "print(\"Finished at \", finish_time)\n",
    "elapsed = finish_time - start_time\n",
    "print(\"Elapsed time: \", elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Troubleshooting per https://stackoverflow.com/questions/57507832/unable-to-allocate-array-with-shape-and-data-type"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14184.439866,
   "end_time": "2021-12-06T11:56:18.179826",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-06T07:59:53.739960",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
